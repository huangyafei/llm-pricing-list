{
    "last_updated": "2024-03-29T08:38:42Z",
    "models":{
        "gpt-4": {
            "context_window": 8192, 
            "max_input_tokens": 8192,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00003,
            "output_cost_per_token": 0.00006,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-4-0613": {
            "context_window": 8192,
            "max_input_tokens": 8192,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00003,
            "output_cost_per_token": 0.00006,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-4-32k": {
            "context_window": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00006,
            "output_cost_per_token": 0.00012,
            "litellm_provider": "openai",
            "mode": "chat"
        },
        "gpt-4-32k-0613": {
            "context_window": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00006,
            "output_cost_per_token": 0.00012,
            "litellm_provider": "openai",
            "mode": "chat"
        },
        "gpt-4-turbo-preview": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-1106-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-0125-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-vision-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "openai",
            "mode": "chat"
        },
        "gpt-4-1106-vision-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "openai",
            "mode": "chat"
        },
        "gpt-3.5-turbo": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000005,
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-3.5-turbo-0613": {
            "context_window": 4096,
            "max_input_tokens": 4096,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000015,
            "output_cost_per_token": 0.000002,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-3.5-turbo-1106": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000010,
            "output_cost_per_token": 0.0000020,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-3.5-turbo-0125": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000005,
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "openai",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-3.5-turbo-16k": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000004,
            "litellm_provider": "openai",
            "mode": "chat"
        },
        "gpt-3.5-turbo-16k-0613": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000004,
            "litellm_provider": "openai",
            "mode": "chat"
        },
        "gpt-3.5-turbo-instruct": {
            "context_window": 4096,
            "max_input_tokens": 4096,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000015,
            "output_cost_per_token": 0.000002,
            "litellm_provider": "openai",
            "mode": "completion"
        },
        "text-embedding-3-small": {
            "max_tokens": 8191,
            "max_input_tokens": 8191,
            "input_cost_per_token": 0.00000002,
            "output_cost_per_token": 0.000000,
            "litellm_provider": "openai",
            "mode": "embedding"
        },
        "text-embedding-3-large": {
            "max_tokens": 8191,
            "max_input_tokens": 8191,
            "input_cost_per_token": 0.00000013,
            "output_cost_per_token": 0.000000,
            "litellm_provider": "openai",
            "mode": "embedding"
        },
        "text-embedding-ada-002": {
            "max_tokens": 8191,
            "max_input_tokens": 8191,
            "input_cost_per_token": 0.0000001,
            "output_cost_per_token": 0.000000,
            "litellm_provider": "openai",
            "mode": "embedding"
        },
        "text-moderation-latest": {
            "max_tokens": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 0,
            "input_cost_per_token": 0.000000,
            "output_cost_per_token": 0.000000,
            "litellm_provider": "openai",
            "mode": "moderations"
        },
        "text-moderation-stable": {
            "max_tokens": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 0,
            "input_cost_per_token": 0.000000,
            "output_cost_per_token": 0.000000,
            "litellm_provider": "openai",
            "mode": "moderations"
        },
        "text-moderation-007": {
            "max_tokens": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 0,
            "input_cost_per_token": 0.000000,
            "output_cost_per_token": 0.000000,
            "litellm_provider": "openai",
            "mode": "moderations"
        },
        "whisper-1": {
            "mode": "audio_transcription",
            "input_cost_per_second": 0,
            "output_cost_per_second": 0.0001, 
            "litellm_provider": "openai"
        },
        "claude-instant-1": {
            "context_window": 100000,
            "max_input_tokens": 100000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000008,
            "output_cost_per_token": 0.0000024,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "claude-instant-1.2": {
            "context_window": 100000,
            "max_input_tokens": 100000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000008,
            "output_cost_per_token": 0.0000024,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "claude-2": {
            "context_window": 100000,
            "max_input_tokens": 100000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000008,
            "output_cost_per_token": 0.000024,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "claude-2.1": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000008,
            "output_cost_per_token": 0.000024,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "claude-3-haiku-20240307": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00000025,
            "output_cost_per_token": 0.00000125,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "claude-3-opus-20240229": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000015,
            "output_cost_per_token": 0.000075,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "claude-3-sonnet-20240229": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000015,
            "litellm_provider": "anthropic",
            "mode": "chat"
        },
        "gemini/gemini-1.0-pro": {
            "context_window": 32760,
            "max_input_tokens": 32760,
            "max_output_tokens": 8192,
            "input_cost_per_token": 0.0000005, 
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "gemini",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        },
        "gemini/gemini-1.0-pro-vision": {
            "context_window": 16384,
            "max_input_tokens": 16384,
            "max_output_tokens": 2048,
            "input_cost_per_token": 0.0000005, 
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "gemini",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        },
        "gemini/gemini-1.5-pro": {
            "context_window": 1000000,
            "max_input_tokens": 1000000,
            "max_output_tokens": 8192,
            "input_cost_per_token": 0.000007, 
            "output_cost_per_token": 0.000021,
            "litellm_provider": "gemini",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        },
         "gemini/gemini-1.5-pro-vision": {
            "context_window": 1000000,
            "max_input_tokens": 1000000,
            "max_output_tokens": 8192,
            "input_cost_per_token": 0.000007, 
            "output_cost_per_token": 0.000021,
            "litellm_provider": "gemini",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        }
    }
}