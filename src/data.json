{
    "last_updated": "2024-07-19T10:37:24Z",
    "models":{
        "gpt-4": {
            "context_window": 8192, 
            "max_input_tokens": 8192,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00003,
            "output_cost_per_token": 0.00006,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-4-0613": {
            "context_window": 8192,
            "max_input_tokens": 8192,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00003,
            "output_cost_per_token": 0.00006,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-4-32k": {
            "context_window": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00006,
            "output_cost_per_token": 0.00012,
            "litellm_provider": "OpenAI",
            "mode": "chat"
        },
        "gpt-4-32k-0613": {
            "context_window": 32768,
            "max_input_tokens": 32768,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00006,
            "output_cost_per_token": 0.00012,
            "litellm_provider": "OpenAI",
            "mode": "chat"
        },
        "gpt-4o": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.000005,
            "output_cost_per_token": 0.000015,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4o-2024-05-13": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.000005,
            "output_cost_per_token": 0.000015,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4o-mini": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00000015,
            "output_cost_per_token": 0.00000060,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4o-mini-2024-07-18": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00000015,
            "output_cost_per_token": 0.00000060,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-turbo": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-turbo-2024-04-09": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-turbo-preview": {
            "context_window": 128000, 
            "max_input_tokens": 128000,
            "max_output_tokens": 4096, 
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-1106-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-0125-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-4-vision-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat"
        },
        "gpt-4-1106-vision-preview": {
            "context_window": 128000,
            "max_input_tokens": 128000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00003,
            "litellm_provider": "OpenAI",
            "mode": "chat"
        },
        "gpt-3.5-turbo": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000005,
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-3.5-turbo-0613": {
            "context_window": 4096,
            "max_input_tokens": 4096,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000015,
            "output_cost_per_token": 0.000002,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true
        },
        "gpt-3.5-turbo-1106": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000010,
            "output_cost_per_token": 0.0000020,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-3.5-turbo-0125": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000005,
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "OpenAI",
            "mode": "chat",
            "supports_function_calling": true,
            "supports_parallel_function_calling": true
        },
        "gpt-3.5-turbo-16k": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000004,
            "litellm_provider": "OpenAI",
            "mode": "chat"
        },
        "gpt-3.5-turbo-16k-0613": {
            "context_window": 16385,
            "max_input_tokens": 16385,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000004,
            "litellm_provider": "OpenAI",
            "mode": "chat"
        },
        "gpt-3.5-turbo-instruct": {
            "context_window": 4096,
            "max_input_tokens": 4096,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000015,
            "output_cost_per_token": 0.000002,
            "litellm_provider": "OpenAI",
            "mode": "completion"
        },
        "claude-instant-1": {
            "context_window": 100000,
            "max_input_tokens": 100000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000008,
            "output_cost_per_token": 0.0000024,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-instant-1.2": {
            "context_window": 100000,
            "max_input_tokens": 100000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.0000008,
            "output_cost_per_token": 0.0000024,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-2": {
            "context_window": 100000,
            "max_input_tokens": 100000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000008,
            "output_cost_per_token": 0.000024,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-2.1": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000008,
            "output_cost_per_token": 0.000024,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-3-haiku-20240307": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.00000025,
            "output_cost_per_token": 0.00000125,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-3-opus-20240229": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000015,
            "output_cost_per_token": 0.000075,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-3-sonnet-20240229": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000015,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "claude-3-5-sonnet-20240620": {
            "context_window": 200000,
            "max_input_tokens": 200000,
            "max_output_tokens": 4096,
            "input_cost_per_token": 0.000003,
            "output_cost_per_token": 0.000015,
            "litellm_provider": "Anthropic",
            "mode": "chat"
        },
        "gemini/gemini-1.0-pro": {
            "context_window": 32760,
            "max_input_tokens": 32760,
            "max_output_tokens": 8192,
            "input_cost_per_token": 0.0000005, 
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "Google",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        },
        "gemini/gemini-1.0-pro-vision": {
            "context_window": 16384,
            "max_input_tokens": 16384,
            "max_output_tokens": 2048,
            "input_cost_per_token": 0.0000005, 
            "output_cost_per_token": 0.0000015,
            "litellm_provider": "Google",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        },
        "gemini/gemini-1.5-pro": {
            "context_window": 1000000,
            "max_input_tokens": 1000000,
            "max_output_tokens": 8192,
            "input_cost_per_token": 0.000007, 
            "output_cost_per_token": 0.000021,
            "litellm_provider": "Google",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        },
         "gemini/gemini-1.5-pro-vision": {
            "context_window": 1000000,
            "max_input_tokens": 1000000,
            "max_output_tokens": 8192,
            "input_cost_per_token": 0.000007, 
            "output_cost_per_token": 0.000021,
            "litellm_provider": "Google",
            "mode": "chat",
            "supports_function_calling": true,
            "source": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
        }
    }
}